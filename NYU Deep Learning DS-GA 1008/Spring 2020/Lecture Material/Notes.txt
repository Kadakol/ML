Week 1 Lecture - Yann LeCun:

Inspiration for DL - The brain

McCulloch and Pitts - Network of binary neurons, 
Adaline - weights are electrochemical memistors
Hebb - Hebbian learning, neurons which fire together - increase synaptic strength and vice-versa
Wiener - Cybernetics, sensors - actuators - feedback loops (car steering example)
Rosenblatt - perceptron, a physical analog computer, optical sensors as well
Hubel & Wiesel - visual cortex architecture

1968 to 1984 - AI winter

1985 - backprop
Backprop works because activation function is continuous and differentiable
Floating point computation before '85 was difficult. Hence binary neurons were used. Thus backprop could not have worked.

1995 to 2010 - AI winter again

Started with speech recog around 2010
Around 2012, Android phones were using NN for speech recog
2012 onwards, ImageNet/CV
2016 onwards - NLP, language translation
Next - robotics, control etc.

Supervised Learning - 90%+ approaches

Perceptron - 2 layers - 2nd layer trainable, 1st layer fixed (associative layer)

Pattern recognition: 
	Traditional ML:
		image -> feature extractor (hand engineered) -> trainable classifier (trainable)
		Feature extractor produce a vector
		Feed the vector to a trainable classifier - Weighted sum + threshold
		More focus was on feature extractor, less on the classifier

	DL:
		Trainable low-level, mid-level, high-level features, classifier
		Multiple layers = deep
		Non-linearity - 2 successive linear functions can be represented as a single linear function, so no point having multiple layers
		
Simplest non-linearity - ReLU/half wave rectification/positive part
Lot of 0s generated
Vector - Multiply by matrix (learnable) - Get a vector which is a weighted sum - Pass through non-linearity - repeat

Function optimization - SGD
Find the direction of minima and take a step
Convergence guaranteed if function is convex, otherwise may get stuck in local minima
SGD - empirically, converges much faster if you have a large training set; better generalization than GD
Because of redundancy, using batch gradient descent will take a long time to converge. Samples may be repeated. So before computing the gradient, you will see all the samples hundreds of times which will add no value.
SGD may be more efficient because updates happen immediately

Backprop - practical application of chain rule

To deal with images - Hubel and Wiesel - Visual cortex
Eye model - retina - compresses the image from 100M to 1M - optic nerve is about 1M
Some neurons turn on for particular orientations of edges etc. - focus only on a small area of the visual field

Complex cells - pools activation
Simple cells - activation changes for translation etc.; detects local features
Complex cells become agnostic to this due to pooling

CNN - LeCun 1989

LeNet at Bell AT&T Labs 
Detect objects regardless of size - move a window, wherever it activates a face is present. if window is smaller than face, subsample the image and move the window again and so on.

Training a robot to drive itself

Semantic segmentation

MobileEye - first company to use CNNs/vision based system for self driving

NN Accelerators - hottest topics now

2010 - speech recog
2013 - image recog
2015 - NLP
 
2012 - Alex Krizhevsky - Geoff Hinton's student - implemented in GPUs
ImageNet - 1.3M samples
LeNet - AlexNet - VGG - GoogLeNet - ResNet
ResNet mostly used nowadays

Natural data is compositional - efficiently representable hierarchy - low-level, mid-level, high-level features...

MaskRCNN, RetinaNet, Feature Pyramid Network

Detectron2 - panoptic instance segmentation

MobileEye + NVIDIA -> Tesla driving assistance
AEBS - Advanced Emergency Braking Systems

3D ConvNet for MRI Images

Any function can be approximated with 2 layers - why the need for layers?
What is special about CNN - why do they work so well on natural signals?
Objective functions are highly non-convex - why doesn't SGD get trapped in local minima?
Networks are overparameterized - why don't they overfit?

Learning Representations
Basic principle - expanding the dimension of the representation so that things are more likely to be linearly separable.
	Space tiling
	random projections
	polynomial classifiers
	radial basis functions
	kernel machines
	
Extreme Learning Machines - 2 layer networks, where the 1st layer is random

Hierarchical representation

SVM - 2 layers NN
1st layer, a function which compares all elements pair-wise - get a distance between each pair
Produces scores
Weighted sum of scores
Learn the alphas / weights
2nd layer is trainable
1st layer is trained unsupervised only using the X, not the Y
Store every single X and use that as the weight of a neuron
So 1st layer is trained unsupervised and the 2nd layer is a linear classifier
Glorified template matching!!

Doesn't work for images

So why do we need deep networks when any function can be approximated with 2 layers?
	What kind of kernel function to use?
	Too many terms.
	Not easy to solve.
	
	If the output of the 1st layer is large enough (approaching infinity), any function can be approximated.
	
	Very expensive to do in 2 layers
	
	Deep architectures trade space for time / breadth for depth
	Eg., N bit input - number of bits on is even or odd?
		Middle layer needs exponential terms if done in 2 layers
		Instead, go deeper. log(N) layers. Can be done in linear complexity. 
		Go from exponential to linear by using multiple layers.
		
	Eg. add 2 binary numbers - add each bit, carry, add next bit and so on... #ops proportional to number of bits N. Or, use carry look-ahead and reduce N, but higher complexity/more area on chip.
	
2 layers are not deep
	No hierarchy
	Doesn't learn complex representations

1 hidden layer / SVM / kernel machines / Decision trees are not deep 
	No hierarchy
	
	
What are good features / representations?
Manifold hypothesis
	Natural data lives in low-dimensional manifold in the large dimension N-d space
	Variable in natural data are mutually dependent.
	Picture of a face
	Number of degrees of freedom - number of muscles in the face / translation in 3D / rotation / tilt etc.
	About 50, not more than 100
	Out of 1M pixels (3M space), going from one picture to another, not more than 100 parameters will determine the position of a point on the surface.
	
A feature extractor should disentagle all of these relations.
Find out the contribution of each feature independently.
No one knows how exactly to do this.

PCA can find the relations if the manifold is linear.






================================================================================================================================







Week 2 Lecture - Yann LeCun:

Parameterized models - functions that depend on 2 parameters - input and a trainable parameter
Parameters are shared across training samples
In DL - parameters are stored inside, no need to pass them explicitly, in general

Cost function - compare output of module with output you want

Examples of parameterized functions - linear regression, nearest neighbor

Loss function - average loss

ML - typically minimize functions, sometimes maximize and sometimes find the Nash equilibrium between 2 functions (eg. GANs)
Done using gradient-based methods - a method that finds the minimum of a function assuming it is easy to calculate the gradient of that function - assumes that the function is continuous and differentiable almost everywhere - OK to have small kinks in it (eg. ReLU)

Gradient free methods or zeroth order methods - non-gradient-based optimization methods
Examples - a golf course with a hole in it - no useful information at every step except for a small area; staircase like methods - again no useful information everywhere; where the function is not known, where the function might be the entire environment - not efficient to compute the gradient

DL is all about gradient-based methods

RL - uses gradient estimation without the gradient - cost function is not differentiable most of the times, but the network that computes the output that goes into the environment is differentiable - differentiable from that point on. Cost function - give it a y_hat and it tells you the cost value, but not the gradient. There is no y.
RL is hugely inefficient - no y, no correct answer, just keep trying things. 

SGD - for each sample, calculate the cost, find the gradient and take a step in the negative direction of the gradient - very noisy - on average, you'll move towards the minimum. Due to redundancy in samples, SGD is faster.
Batch gradient descent - GPU is more efficient if you have batches, easily parallellize. No algorithmic superiority!
Million samples - 10k samples repeated 100 times - redundancy. For full batch GD, all computations must be completed before you take a step. Due to redundancy of samples, this is wasted computation. For SGD, since a step is taken each time, by the time the samples start repeating, you've already seen all the data. Hence, more efficient.

Good amount of batching to be efficient and to not have too much redundancy?
Empirical - number of samples in a batch should be roughly equal (1x or 2x) to the number of cateogries (in case of classification). e.g., ImageNet - 1k classes, batch size could be 2k.
If there is any possibility of generalization, there has to be redundancy!

Traditional neural nets
Interspersed operation of linear operations and pointwise non-linear operations
If no non-linearities - might as well have a single layer - product of 2 linear equations is a linear equation - product of 2 matries is a single matrix

Backprop
Chain rule
Small perturbation of input -> how much variation in output?
Apply for non-linear and linear functions

For modules - compute the Jacobian matrix and keep multiplying it during backprop.
2 Jacobian matrices per module - with respect to z and with respect to w.

LogSoftmax
yi = e^xi / sum(e^xj)
log yi = xi - log(sum(e^xj))

What is dC/dxi?
dC/dxi = dC/dyi dyi/dxi
dyi/dxi = 1 - e^xi / sum(e^xi)

Sigmoid output - y_bar - cost function compares with y
Use squared error ||y_bar - y||^2 - why wont it work?
In order to get sigmoid output 1 or 0, weights have to be very large +ve or -ve. Backprop in these regions not possible due to saturation. Gradient is almost 0.
Instead, set the targets at 0.8 and 0.2 - this way the weights wont go to infinity
Or, take the log of it.
Sigmoid is a specific case of softmax - where one input is s and the other input is 0


Backprop in practice:
Use ReLU over tanh and sigmoid
Cross-entropy for classification
Use SGD on minibatches
Shuffle the training samples
Normalize the input - 0 mean, unit variance
Use LR decay
Turn on L1 or L2 (or both) regularizarion on weights after a couple of epochs - initially weight initialization is a saddle point. Turing L1 or L2 on initially will push weights to 0 and then nothing will work.
Use dropout for regularizarion
Read paper Efficient Backprop
Neural Networks, Tricks of the Trade - Book

Cross-entropy loss function expects LogSoftmax, not Softmax

RGB image - 3 separate channels
Find the mean across all the values in blue - m b
Find the standard deviation across all values in blue - sigma b
Similarly for red and green - m r, sigma r, m g, sigma g
6 scalar values found
For each value in the r channel, subtract by mean and divide by standard deviation (or max(sigma, epsilon)). Similarly for green and blue. Doing this per channel is better. Gets rid of global illumination. E.g., sunlight low blue amplitude, underwater low red amplitude.
Normalizes the contrast and makes the variance 0

Weight decay - minimize the cost function but also do it with smallest weights possible - L2

Lasso regularizarion - Bring the weight values to 0 if it is not useful - L1

Initialization is important!
Kaiming initialization - weights to have 0 mean and variance to be proportional to the inverse of the sqrt of the number of inputs

Any directed acyclic graph is OK for backprop






================================================================================================================================







Week 3 Lecture - Yann LeCun:

Alf's demo:
Classification - deep network with only 2 neurons per layer
Rotation, reflection, scaling/zooming, translation from the affine transformations + bias + non-linearity to get rid of the negative part - repeat a few times until points become linearity separable

Weight sharing - two weights are forced to be equal to each other

Hypernetwork - Weights are computed as a function of X, which are then applied to transform X to produce Y

Shared weights for motif detection
Same weights applied on different sub-sequences of the signal - go to a max function - this is the output
E.g., wakework detection - Alexa - within a given sequence, the word Alexa is present somewhere, doesn't matter where.

During backprop, compute gradients for each of the copies of the transformation function and sum it up, since the same set of weights are being used for all of the transformation functions.
PyTorch - handles this accumulation implicitly. Hence, need to zero out gradients at each step.

C and D detector - detect endpoints and edges

Equivariance to shift - output will shift since the input is shifted, but no other changes
Invariance to shift - no change in the output due to shift

Learn these templates as weights of a NN

Discrete convolution / Cross-correlation

Conv - non-linearity - pooling
stride, padding

Layers - hierarchical representations of natural data

How the brain interprets images
One pathway for WHAT, another pathway for WHERE

Hubel and Wiesel - repeat from lecture 1 - simple and complex cells

First convnets
Tanh activation

As you go deeper, due to pooling and subsampling, amount of shift in the feature map due to shift in the input keeps decreasing. This makes it easier for the later layers to interpret what is present in the image - shift equivariance

Normalization - batch norm, group norm
Pooling - max, Lp norm, log prob
Max = max(Xi); Lp = sum(Xi ^ p) ^ 1/p ; LogProb = (1/b)log(sum(e^bXi))
A good pooling operation is one which gives the same result irrespective of the permutation of the input

Multiple character recognition
If network is fully convolutional, changing the size of the input will make the network adapt such that the size of the output changes.
Cursive handwriting - take a 32x32 window and slide it across each time and get the output - wasteful
Instead, pass the whole image as input and that will produce an output which is proportional to the input image
Train a network to identify individual numbers. At each 32x32 location, with an offset of 4, keep sliding the window until a character is found. This is highly wasteful.
Instead, pass the whole image as input. It produces an output related to the size of the input. This is much cheaper to do than to do repeated convolutions.

"minimum" cursive writing example - in order to detect an object you need to segment it first, but in order to segment it you need to detect it first. So both need to be done together. 

40% change in size may be acceptable for typical CNNs

What are convnets good for?
- Signals with multi-dimension arrays
- Signals with strong local correlations
- Signals where features can appear anywhere
- Signals where objects are invariant to translations and distortions
- 1D ConvNets - sequential signals, text
- 2D ConvNets - images, time-frequency representation (speech and audio)
- 3D ConvNets - video, volumetrics images, tomography images






================================================================================================================================







Week 5 Lecture - Aaron Defazio:

Optimization

Gradient Descent - worst optimization method in the world :D
Iterative
wk+1 = wk - gammak * grad(f(wk))
Assume f(w) is continuous and differentiable, although functions used in DL are not differentiable at every point

Take a step in the negative gradient

Intuition for quadratics
Gradient descent directly cuts off one half of the solution space

How to choose a learning rate?
Too small learning rate - slow convergence
Too large - can lead to divergence
Perfect learning rate - may only be possible to find in quadratics, for example - go to the solution directly
Slightly large but not too large learning rate - faster convergence, but more zig-zagging - choose a rate which is at the edge of divergence

Stochastic optimization (why a rough estimate now is better than a good estimate in a week)
Instead of the gradient, use the stochastic approximation of the gradient in GD
Instead of full gradient, use the stochastic gradient
GD - worst method - use the full dataset to calculate the gradient
SGD - best method - calculate the gradient at each point, for each data sample
SGD = GD + noise

The noise in SGD leads to annealing (prevent convergence to a bad local minima)
Redundancy across instances - noise is lower when redundancy is higher - SGD helps exploit this redundancy
Gradient computed on a few 100 examples is almost as good as the gradient computed on the entire dataset
In early stages of optimization, noise is minimal compared to the information in the gradient. SGD step is almost as good as a GD step.

Mini batches used instead of single instances
Random subset of samples used. Some use with replacement and some use without replacement - not important though
Practical reasons - use batches instead of single samples
ImageNet - use at least 64 batch size
Also, distributed training

Sometimes, you may need to use full-batch for optimization. Don't use GD. In this case, use LBFGS, either from torch.optim.LBFGS or from scipy.optimize.fmin_l_bfgs_b

Why use minibatches which are equal to the number of classes in the dataset? So that the minibatch is representative of the full dataset.

Local minima were a problem 15 years ago, but with SOTA networks this does not seem to matter much. Almost never encounter local minima when recommended parameters are used.

Residual connections - help in smoothening the loss

Momentum:
Always use with SGD
SGD + Momentum = Stochastic heavy ball method
In every step, update p(momemtum) and w(weight)
Dampens the oscillation a bit compared to vanilla SGD
Beta needs to be between 0 and 1. 0 => just SGD, >1 => everything blows up.
Typical values of beta range from 0.25 to 0.99.
Smaller values lead to change in direction quicker
Larger values - slower reaction to change in landscape

Always use momentum - 0.9 or 0.99
Going from 0.9 to 0.99 => reduce learning rate by a factor of 10

Acceleration
Nesterov's momentum vs regular momemtum
By choosing the constants properly, Nesterov's momemtum accelerates convergence on problems with simple structure (convex)
Regular momentum accelerates only on quadratics

Nesterov's momemtum usually performs the same as regular momemtum while training NNs.

Noise smoothing
Momentum smooths the noise from the stochastic gradients.

SGD - mostly used for optimization.

Adaptive methods:
Instead of using the same learning rate across all layers, maintain an estimate of a better rate for each weight separately

RMSProp:
Exponential moving average
Calculate the 2nd moment estimate
Avoid dividing by 0 - put epsilon at the denominator - 10^-7 or 10^-8 - close to the machine epsilon

Adam:
Adaptive moment estimation
RMSProp with momemtum
Momentum improves SGD and RMSProp
Adam uses bias correction, since initially the momemtum buffer is started from all 0s. 

For poorly conditioned problems, Adam works much better than SGD

But Adam has known disadvantages:
- Does not converge for simple problems
- Worse generalization error on CV (e.g. ImageNet)
- More memory than SGD
- 2 momentum parameters, needs some tuning

Try both SGD + momemtum and Adam on different learning rates and use which works better on held out data

Use SGD at the beginning and Adam at the end! Try to avoid this.

Normalization layers:
e.g. Conv -> Batch Norm -> ReLU

y = (a/sigma) (x - mu) + b

a and b are learnable parameters
a = learnable scaling factor
b = learnable bias term
mu = estimate of the mean of the activations
sigma = estimate of the standard deviation of the activations
x = input activations to the normalization layer

Batch Norm, Layer Norm, Group Norm, Instance Norm

Why does normalization help?
Reduces internal covariate shift. Not clearly understood.
- optimization effect
- regularizarion effect
- reduces sensitivity to weight initialization
- allows you to combine almost any neural network building blocks together and it may just work

For Batch Norm, backprop has to be done through the mean and standard deviation. If not done, it diverges.
Recommended - Group Norm - good to use with group size 32
BN and IN - mu and sigma are fixed after training. Not recomputed every time the network is evaluated. For GN, LN - not necessary

The death of optimization:
MRIs capture images in the Fourier domain in terms of rows and columns. Converting from Fourier domain to image domain is a linear transformation and is quite straightforward.
How to accelerate the data capture from MRIs? - skips a few rows and columns while capturing - problem - can't use Fourier Transform / linear operation anymore to go from Fourier domain to image domain

Compressed Sensing - get a perfect reconstruction from subsampled measurements
Assumptions for this to work - need a sparse image - lot of 0s. It can be represented sparsely using a wavelet decomposition.

UNet used to reconstruct the MRI images






================================================================================================================================







Week 6 Lecture - Yann LeCun:

NMS
Deformable parts model
ConvNet with different widths for each classifier - for the last layer
Produce a score at the end
Take an average of scores across these models

Think of it as a graph - shortest path algorithm to find the sequence of characters

Face detection
Collect a dataset - images with faces and images without faces
Problem
- images without faces not representative - couldn't have seen all such cases. So lots of false positives - Handle this using negative mining - go through a large dataset of images which have no faces and see where the detector fires (wrongly). add this patch to the negative cateory and train the detector and repeat the process 4-5 times
- faces may not be of fixed size - take a multiscale approach - downscale by 2 each time and run the detector - 1 + 1/2 + 1/4 + 1/8 + ... = 2 - so not so expensive

Semantic segmentation - every pixel is assigned a category

Robot driving - classify obstacles and areas where the robot can drive

Stereo vision - works upto approx 10 meters. Beyond that it is not reliable.

Pass an image through a convnet and get a set of feature maps
Downsample by 2 and repeat
Downsample by 4 and repeat
Then upsample the 2 and 4 feature maps and concat everything together


Recurrent Nets

Networks with loops - for backprop, unroll the loop
Backprop Through Time - BPTT
Don't work very well in their naive form
Vanishing and exploding gradient problem - if weight matrix is very small, propagating through the weight repeatedly for multiple time steps will lead to almost no gradients at the first time step; conversely for the case with large weights
Purpose of recurrent nets - be able to remember things from the past
Tricks need to be used to make RNNs work

Multiplicative Modules

Networks whose weights are computed by other networks - hypernetworks
Route networks through NNs in a data dependent way
e.g. Attention Module - allows the NN to focus its attention on a particular input and ignores other inputs. Useful in LSTM and GRUs but especially used in all NLP systems

GRUs - Gated Recurrent Units

input, output, update gate, reset gate
Update gate - selectively chooses what to be remembered, either simply the previous hidden state or the current state or some combination of the current input and the previous state

LSTM - Long Short Term Memory

input, forget gate, input gate, output gate, hidden state, cell state

LSTMs were used for speech recog. Now, people prefer to use temporal convnets and Transformers for NLP
One of the first recurrent nets which worked
Used for translation - Neural Machine Translation
Multi-layer LSTM for NMT - input layer LSTM, multiple time steps, take the hidden state of each and pass to a hidden layer LSTM and so on. Finally end up with one vector which represents the whole sentence. Take this vector and pass it to another multi-layer LSTM which produces one word at a time, pass this output along with the previous LSTM hidden state to the next LSTM and so on until you get the full sentence.

Problems:
The whole meaning of the sentence needs to be encoded in one small vector which is passed from the encoder to the decoder.
LSTMs preserve memory for about 20 words. By the time you hit the 20th word, the LSTM would have forgotten the first word. Fix for this is Bi-LSTM - run 2 LSTMs in opposite directions - 2 codes obtained - one from forward sentence running LSTM and the other from the backwards sentence running LSTM - allows doubling the length of the sentence without much loss

Sequence to sequence with attention
Attention decides at a given time step, which of the words to focus on to produce a given output word
Attention is just another set of weights that can be trained using backprop automatically
Attention changed everything. Everyone started focusing on attention and deploying systems with attention especially for NMT.


Memory Networks

End-to-end memory networks
Hippocampus for short term memory
Cortex for long term memory

x - memory address
compare with a bunch of ks - k1, k2 ... using dot product [keys]
Push these comparisons through a softmax
These will give a set of numbers between 0 and 1
Multiply these scalars with another set of vectors v2 - v1, v2.... [values]
Sum them up
This makes an addressable associative memory
Keys and Values can be changed via backprop

Useful for question answering etc.

Example - Input to NN, access memory, get back data to NN, keep repeating and give an output 

Neural Turing machine - similar to above idea, but uses a soft tape for memory

Transfomer is basically a NN where every group of neurons has memory (input, key value described above)






================================================================================================================================







Week 7 Lecture - Yann LeCun:

Energy-based models

Probabilistic models are a special case of energy-based models

Given X, predict a Y - where X and Y can be anything, images, an input image for which a sentence needs to be predicted etc. Fixed number of computations can be performed to get the prediction.
Not applicable if more complicated computation is needed - something more than weighted sums and non-linearity - inference is complex. Also if a set of outputs need to be produced instead of a single output (high dimensional continuous space. e.g., if the output is an image, there is no way to have a set of images as output and do something like a softmax over all of them. Or if the output is text - how to represent a score over all possible text in a compact form?)

Energy-based models can help here. Instead of predicting Y, ask the system if the X and Y are compatible with each other
Inference - Given X, find Y such that F(X, Y) produces a low value
Energy minimization - for e.g., multiclass classification finds a score which has the minimum energy
Find an output that satisfies some constraints, which is defined by F(X, Y)
Energy is minimized during inference, not during learning
Energy-function is a scalar value - low when y is compatible with x and higher when y is less compatible with x
There may be multiple solutions

Feed-forward model is an explicit function which computes y from x
EBM is an implicit function which captures the dependency between x and y - e.g., equation of a circle - Solve x^2 + y^2 - 1 = 0 to get a unit circle
Multiple y can be compatible with a single x

If y is continuous, can use a gradient-based method for inference

When is inference hard?
- high dimensional object structure - sequence, image, video
- output has compositional structure - text, action sequence
- output results from a long chain of reasoning - can be reduced to an optimization problem

Inference involving latent variables
Latent variables are variables whose value is never given to us
e.g.
handwritten word - helps to know where the characters are
speech recog - helps to know where the words and phonemes are

Latent-variable EBM - inference
Simultaneously minimize E w.r.t. y and z 
F_infinity(x,y) = min_z E(x, y, z)
Take a function of x y z, find the minimum of the function over z, z gets eliminated which gives a function of x and y

For a given x, by varying the latent variable z, you can have multiple outputs y
As z varies over a set, y varies over the manifold of possible predictions
Useful for multiple correct/plausible outputs - e.g., video prediction, text generation, translation, image synthesis

Predictor should be able to deal with uncertainity and make multiple predictions and the way the predictions are parameterized is through the latent variable

z is not learnt. It is inferred.

EBM vs probabilistic models
From energy to probability - using Gibbs-Boltzmann distribution
Probabilistic models are a special case of EBMs

Probabilistic models in high dimensional spaces and combinatorial situations (like text) are all approximate models, they're all wrong, and normalizing them makes it more wrong. So it's better to not normalize them. 

Insisting MLE in these cases makes no sense - infinite probability for values on the manifold and 0 probability for all others. Not practical to find such a model. Weights will have to be calibrated such that they are infinite for certain areas and 0 for all others and the integral sums up to 1, which is not possible.


Self-supervised learning

BERT is trained using self-supervised learning - denoising autoencoder
Purpose of self-supervised learning is to train a system to learn good representations of the input, and those representations are subsequently used for another supervised learning task or Reinforcement Learning task

Filling in the blanks - predict a part of the input from any other part
Train a system to learn about what the scene would look like when the camera shifts - it will inherently learn about depth of the image without having to explicitly train it to do so
Background and foreground
Animate and inanimate objects
Objects which are not visible are still there

NLP
Denoising autoencoder
Remove 10-20% of the words and make the system predict those words in a sentence
Train it as a classifier with a big softmax over a set of words which corresponds to a probability distribution over words
After training, throw away the last layer and use the 2nd last layer as a representation of the text which is inputted
Input -> Encoder -> Code -> Decoder 

Image - this doesn't work so great
The representation of the image is not as great as that produced by a supervised learning system trained on ImageNet

NLP is discrete whereas images/videos/signal/audio are continuous
In discrete, able to represent uncertainity using a softmax. Can't do the same for continuous.

How to make high-dimensional predictions under uncertainity - biggest challenge in AI

In a stochastic world, training a system to make a single prediction makes it predict the average of all plausible predictions
E.g., stand a pen and drop it each time at the same place each time - pen falls in different places each time => for the exact same x, many different y. If the system has to make a single prediction, it would be the pen present in all places at the same time.
This is why latent variable EBMs are needed.

The role of z is to parameterize the set of outputs that can occur. In the end, z is supposed to contain information in y which is not present in x.

k-means is an EBM

Training an EBM
Shape F(x,y) such that the data points have lower energy than everything else

2 classes of learning methods:
1. Contrastive methods - Take xi yi and change the parameters of the model such that the energy goes down. Then pick a bad y for all given x and push the energy up. This way the correct y will have lower energy than the bad y. These differ by how you pick the y and the loss function used to push up and down.
2. Architectural methods - build F(x,y) such that the volume of low energy functions is minimized or limited via regularizarion. When you push the energy of data points down, the rest goes up automatically since the volume of stuff which can take low energy is limited.

7 strategies to shape the energy function
C1 - Maximum likelihood - push down the energy of data points to -infinity and all other points to +infinity
Monte Carlo approximation - In order to calculate the expected value of a function, instead of computing it, it can be approximated by computing the average of the gradients for a limited set of samples.
Variational methods - approximate the expectation over a distribution by replacing the distribution with something you can compute and try to make the computable distribution as close as possible to the real distribution using some measure like the KL divergence

K-means - EBM
Clustering algorithm
E(y,z) = ||y - Wz||^2, z E 1 hot
Select one column from W
The columns of W are called prototypes
Goal - minimize the average of the energy over the training set
Architectural methods
Only K points can have 0 energy. The volume of space which can have low energy is fixed.

Most people will use architectural methods
Contrastive methods work well for images right now

Pick a point where the energy is supposed to be low, but is high. Push down the energy on that. Perturb the point a bit and for all those points, push the energy up a bit. This way the energy function curls up around each data point.

Randomly kick the point and do GD. Let it keep following a trajectory. Cut it off at some point. Then push up that point and pull down the original point. 

Random noising - denoising autoencoder
GD with random kick - contrastive divergence; special case of Monte Carlo; Marco Chain Monte Carlo; Hamiltonian Monte Carlo

Denoising Autoencoder
Only have y
Corrupt it. Pass it through an encoder and then a decoder.
Compare the output, which is a reconstruction for y, with y. May just be the distance between y and y_bar
The corruption is not trained. It is only used during training.
This pushes up the energy of the corrupted points
For inference, just pass the y through the encoder-decoder and compare the y_bar with y

During inference, if the input y is on the manifold of the original data, it will be reconstructed as-is. Hence the reconstruction error will be 0.
If the input y is not on the manifold, it will be reconstructed as a closest point on the manifold since that's how it's been trained. Then you get a larger reconstruction error.
The energy computed by the DAE will grow quadratically as the point moves away from the manifold of data
This is how BERT is trained, except the space is discrete since we're dealing with text and the corruption is masking some words and reconstruction is predicting the words which are missing.
A special case of DAE - masked autoencoder - corruption is basically masking




 
















