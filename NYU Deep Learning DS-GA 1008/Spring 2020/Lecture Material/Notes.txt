Week 1 Lecture - Yann LeCun:

Inspiration for DL - The brain

McCulloch and Pitts - Network of binary neurons, 
Adaline - weights are electrochemical memistors
Hebb - Hebbian learning, neurons which fire together - increase synaptic strength and vice-versa
Wiener - Cybernetics, sensors - actuators - feedback loops (car steering example)
Rosenblatt - perceptron, a physical analog computer, optical sensors as well
Hubel & Wiesel - visual cortex architecture

1968 to 1984 - AI winter

1985 - backprop
Backprop works because activation function is continuous and differentiable
Floating point computation before '85 was difficult. Hence binary neurons were used. Thus backprop could not have worked.

1995 to 2010 - AI winter again

Started with speech recog around 2010
Around 2012, Android phones were using NN for speech recog
2012 onwards, ImageNet/CV
2016 onwards - NLP, language translation
Next - robotics, control etc.

Supervised Learning - 90%+ approaches

Perceptron - 2 layers - 2nd layer trainable, 1st layer fixed (associative layer)

Pattern recognition: 
	Traditional ML:
		image -> feature extractor (hand engineered) -> trainable classifier (trainable)
		Feature extractor produce a vector
		Feed the vector to a trainable classifier - Weighted sum + threshold
		More focus was on feature extractor, less on the classifier

	DL:
		Trainable low-level, mid-level, high-level features, classifier
		Multiple layers = deep
		Non-linearity - 2 successive linear functions can be represented as a single linear function, so no point having multiple layers
		
Simplest non-linearity - ReLU/half wave rectification/positive part
Lot of 0s generated
Vector - Multiply by matrix (learnable) - Get a vector which is a weighted sum - Pass through non-linearity - repeat

Function optimization - SGD
Find the direction of minima and take a step
Convergence guaranteed if function is convex, otherwise may get stuck in local minima
SGD - empirically, converges much faster if you have a large training set; better generalization than GD
Because of redundancy, using batch gradient descent will take a long time to converge. Samples may be repeated. So before computing the gradient, you will see all the samples hundreds of times which will add no value.
SGD may be more efficient because updates happen immediately

Backprop - practical application of chain rule

To deal with images - Hubel and Wiesel - Visual cortex
Eye model - retina - compresses the image from 100M to 1M - optic nerve is about 1M
Some neurons turn on for particular orientations of edges etc. - focus only on a small area of the visual field

Complex cells - pools activation
Simple cells - activation changes for translation etc.; detects local features
Complex cells become agnostic to this due to pooling

CNN - LeCun 1989

LeNet at Bell AT&T Labs 
Detect objects regardless of size - move a window, wherever it activates a face is present. if window is smaller than face, subsample the image and move the window again and so on.

Training a robot to drive itself

Semantic segmentation

MobileEye - first company to use CNNs/vision based system for self driving

NN Accelerators - hottest topics now

2010 - speech recog
2013 - image recog
2015 - NLP
 
2012 - Alex Krizhevsky - Geoff Hinton's student - implemented in GPUs
ImageNet - 1.3M samples
LeNet - AlexNet - VGG - GoogLeNet - ResNet
ResNet mostly used nowadays

Natural data is compositional - efficiently representable hierarchy - low-level, mid-level, high-level features...

MaskRCNN, RetinaNet, Feature Pyramid Network

Detectron2 - panoptic instance segmentation

MobileEye + NVIDIA -> Tesla driving assistance
AEBS - Advanced Emergency Braking Systems

3D ConvNet for MRI Images

Any function can be approximated with 2 layers - why the need for layers?
What is special about CNN - why do they work so well on natural signals?
Objective functions are highly non-convex - why doesn't SGD get trapped in local minima?
Networks are overparameterized - why don't they overfit?

Learning Representations
Basic principle - expanding the dimension of the representation so that things are more likely to be linearly separable.
	Space tiling
	random projections
	polynomial classifiers
	radial basis functions
	kernel machines
	
Extreme Learning Machines - 2 layer networks, where the 1st layer is random

Hierarchical representation

SVM - 2 layers NN
1st layer, a function which compares all elements pair-wise - get a distance between each pair
Produces scores
Weighted sum of scores
Learn the alphas / weights
2nd layer is trainable
1st layer is trained unsupervised only using the X, not the Y
Store every single X and use that as the weight of a neuron
So 1st layer is trained unsupervised and the 2nd layer is a linear classifier
Glorified template matching!!

Doesn't work for images

So why do we need deep networks when any function can be approximated with 2 layers?
	What kind of kernel function to use?
	Too many terms.
	Not easy to solve.
	
	If the output of the 1st layer is large enough (approaching infinity), any function can be approximated.
	
	Very expensive to do in 2 layers
	
	Deep architectures trade space for time / breadth for depth
	Eg., N bit input - number of bits on is even or odd?
		Middle layer needs exponential terms if done in 2 layers
		Instead, go deeper. log(N) layers. Can be done in linear complexity. 
		Go from exponential to linear by using multiple layers.
		
	Eg. add 2 binary numbers - add each bit, carry, add next bit and so on... #ops proportional to number of bits N. Or, use carry look-ahead and reduce N, but higher complexity/more area on chip.
	
2 layers are not deep
	No hierarchy
	Doesn't learn complex representations

1 hidden layer / SVM / kernel machines / Decision trees are not deep 
	No hierarchy
	
	
What are good features / representations?
Manifold hypothesis
	Natural data lives in low-dimensional manifold in the large dimension N-d space
	Variable in natural data are mutually dependent.
	Picture of a face
	Number of degrees of freedom - number of muscles in the face / translation in 3D / rotation / tilt etc.
	About 50, not more than 100
	Out of 1M pixels (3M space), going from one picture to another, not more than 100 parameters will determine the position of a point on the surface.
	
A feature extractor should disentagle all of these relations.
Find out the contribution of each feature independently.
No one knows how exactly to do this.

PCA can find the relations if the manifold is linear.






================================================================================================================================







Week 2 Lecture - Yann LeCun:

Parameterized models - functions that depend on 2 parameters - input and a trainable parameter
Parameters are shared across training samples
In DL - parameters are stored inside, no need to pass them explicitly, in general

Cost function - compare output of module with output you want

Examples of parameterized functions - linear regression, nearest neighbor

Loss function - average loss

ML - typically minimize functions, sometimes maximize and sometimes find the Nash equilibrium between 2 functions (eg. GANs)
Done using gradient-based methods - a method that finds the minimum of a function assuming it is easy to calculate the gradient of that function - assumes that the function is continuous and differentiable almost everywhere - OK to have small kinks in it (eg. ReLU)

Gradient free methods or zeroth order methods - non-gradient-based optimization methods
Examples - a golf course with a hole in it - no useful information at every step except for a small area; staircase like methods - again no useful information everywhere; where the function is not known, where the function might be the entire environment - not efficient to compute the gradient

DL is all about gradient-based methods

RL - uses gradient estimation without the gradient - cost function is not differentiable most of the times, but the network that computes the output that goes into the environment is differentiable - differentiable from that point on. Cost function - give it a y_hat and it tells you the cost value, but not the gradient. There is no y.
RL is hugely inefficient - no y, no correct answer, just keep trying things. 

SGD - for each sample, calculate the cost, find the gradient and take a step in the negative direction of the gradient - very noisy - on average, you'll move towards the minimum. Due to redundancy in samples, SGD is faster.
Batch gradient descent - GPU is more efficient if you have batches, easily parallellize. No algorithmic superiority!
Million samples - 10k samples repeated 100 times - redundancy. For full batch GD, all computations must be completed before you take a step. Due to redundancy of samples, this is wasted computation. For SGD, since a step is taken each time, by the time the samples start repeating, you've already seen all the data. Hence, more efficient.

Good amount of batching to be efficient and to not have too much redundancy?
Empirical - number of samples in a batch should be roughly equal (1x or 2x) to the number of cateogries (in case of classification). e.g., ImageNet - 1k classes, batch size could be 2k.
If there is any possibility of generalization, there has to be redundancy!

Traditional neural nets
Interspersed operation of linear operations and pointwise non-linear operations
If no non-linearities - might as well have a single layer - product of 2 linear equations is a linear equation - product of 2 matries is a single matrix

Backprop
Chain rule
Small perturbation of input -> how much variation in output?
Apply for non-linear and linear functions

For modules - compute the Jacobian matrix and keep multiplying it during backprop.
2 Jacobian matrices per module - with respect to z and with respect to w.

LogSoftmax
yi = e^xi / sum(e^xj)
log yi = xi - log(sum(e^xj))

What is dC/dxi?
dC/dxi = dC/dyi dyi/dxi
dyi/dxi = 1 - e^xi / sum(e^xi)

Sigmoid output - y_bar - cost function compares with y
Use squared error ||y_bar - y||^2 - why wont it work?
In order to get sigmoid output 1 or 0, weights have to be very large +ve or -ve. Backprop in these regions not possible due to saturation. Gradient is almost 0.
Instead, set the targets at 0.8 and 0.2 - this way the weights wont go to infinity
Or, take the log of it.
Sigmoid is a specific case of softmax - where one input is s and the other input is 0


Backprop in practice:
Use ReLU over tanh and sigmoid
Cross-entropy for classification
Use SGD on minibatches
Shuffle the training samples
Normalize the input - 0 mean, unit variance
Use LR decay
Turn on L1 or L2 (or both) regularizarion on weights after a couple of epochs - initially weight initialization is a saddle point. Turing L1 or L2 on initially will push weights to 0 and then nothing will work.
Use dropout for regularizarion
Read paper Efficient Backprop
Neural Networks, Tricks of the Trade - Book

Cross-entropy loss function expects LogSoftmax, not Softmax

RGB image - 3 separate channels
Find the mean across all the values in blue - m b
Find the standard deviation across all values in blue - sigma b
Similarly for red and green - m r, sigma r, m g, sigma g
6 scalar values found
For each value in the r channel, subtract by mean and divide by standard deviation (or max(sigma, epsilon)). Similarly for green and blue. Doing this per channel is better. Gets rid of global illumination. E.g., sunlight low blue amplitude, underwater low red amplitude.
Normalizes the contrast and makes the variance 0

Weight decay - minimize the cost function but also do it with smallest weights possible - L2

Lasso regularizarion - Bring the weight values to 0 if it is not useful - L1

Initialization is important!
Kaiming initialization - weights to have 0 mean and variance to be proportional to the inverse of the sqrt of the number of inputs

Any directed acyclic graph is OK for backprop






================================================================================================================================







Week 3 Lecture - Yann LeCun:

Alf's demo:
Classification - deep network with only 2 neurons per layer
Rotation, reflection, scaling/zooming, translation from the affine transformations + bias + non-linearity to get rid of the negative part - repeat a few times until points become linearity separable

Weight sharing - two weights are forced to be equal to each other

Hypernetwork - Weights are computed as a function of X, which are then applied to transform X to produce Y

Shared weights for motif detection
Same weights applied on different sub-sequences of the signal - go to a max function - this is the output
E.g., wakework detection - Alexa - within a given sequence, the word Alexa is present somewhere, doesn't matter where.

During backprop, compute gradients for each of the copies of the transformation function and sum it up, since the same set of weights are being used for all of the transformation functions.
PyTorch - handles this accumulation implicitly. Hence, need to zero out gradients at each step.

C and D detector - detect endpoints and edges

Equivariance to shift - output will shift since the input is shifted, but no other changes
Invariance to shift - no change in the output due to shift

Learn these templates as weights of a NN

Discrete convolution / Cross-correlation

Conv - non-linearity - pooling
stride, padding

Layers - hierarchical representations of natural data

How the brain interprets images
One pathway for WHAT, another pathway for WHERE

Hubel and Wiesel - repeat from lecture 1 - simple and complex cells

First convnets
Tanh activation

As you go deeper, due to pooling and subsampling, amount of shift in the feature map due to shift in the input keeps decreasing. This makes it easier for the later layers to interpret what is present in the image - shift equivariance

Normalization - batch norm, group norm
Pooling - max, Lp norm, log prob
Max = max(Xi); Lp = sum(Xi ^ p) ^ 1/p ; LogProb = (1/b)log(sum(e^bXi))
A good pooling operation is one which gives the same result irrespective of the permutation of the input

Multiple character recognition
If network is fully convolutional, changing the size of the input will make the network adapt such that the size of the output changes.
Cursive handwriting - take a 32x32 window and slide it across each time and get the output - wasteful
Instead, pass the whole image as input and that will produce an output which is proportional to the input image
Train a network to identify individual numbers. At each 32x32 location, with an offset of 4, keep sliding the window until a character is found. This is highly wasteful.
Instead, pass the whole image as input. It produces an output related to the size of the input. This is much cheaper to do than to do repeated convolutions.

"minimum" cursive writing example - in order to detect an object you need to segment it first, but in order to segment it you need to detect it first. So both need to be done together. 

40% change in size may be acceptable for typical CNNs

What are convnets good for?
- Signals with multi-dimension arrays
- Signals with strong local correlations
- Signals where features can appear anywhere
- Signals where objects are invariant to translations and distortions
- 1D ConvNets - sequential signals, text
- 2D ConvNets - images, time-frequency representation (speech and audio)
- 3D ConvNets - video, volumetrics images, tomography images









