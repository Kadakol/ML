Week 1 Practicum - Alfredo Canziani:

Linear algebra

G Strang book
3blue1brown

Matrix multiplication:
Linear transformations:
	Rotation
	Scaling/Stretching
	Reflection
	Shearing
	Translation
	
Affine Transformations
	
Translation
Scaling - using diagonal matrices

Using matrices and scalars to move things around and zoom

Singular Value Decomposition

Branches of a spiral - example
2D plane
(x, y)
color is a 3rd dimension - defines which class the arms of the spiral belong to

Stretch the space such that all arms become linearly separable
Finally at convergence, we get a matrix with N rows (where N represents the number of classes)
In this case, i/p dimension is 2 (2d space). So number of columns = 2. 5 classes output, so number of rows = 5.

NNs take the space fabric and apply a transformation, which is still parameterized by matrices

Why do we need many matrices and non-linearities?
	Here, we need 2 matrices
	Input matrix = 2
	Intermediate layer = 100
	Non-linearity = positive part
	Output layer = 5
	
Without non-linearities - looks just like a 2 layer network. And 2 layer networks can only do scaling, translation, rotation, reflection and shearing






================================================================================================================================






Week 2 Practicum - Alfredo Canziani:

Neural nets - used for rotation and squashing

ANN - supervised learning - classification

3 branches of a spiral
R2 - 2D space
colors - labels

Logistic regression - linear decision boundaries
points cut across decision boundaries. Not linearly separable.

Training data:
X E R^n

X = m x n (data - m rows, n columns. In this case, n=2 for 2D space. So m x 2)
c = Column vector - m x 1 - c1, c2..., cm, where ci E {1, 2, ... K}. Here, there are 3 classes. So K=3.
Y = 1-hot encoded values of c - m x K. The 3 classes here are 100, 010, 001. So Y is of dim m x 3.

FC connected layer
         h
x -> f -> g -> y^
  Wh   Wg

h = f(Wh x + bh)
y^ = g(Wy h + by)

Affine transformations = rotations
Non-linear functions = squashing

f and g are non-linear functions

Rn -> Rd -> Rk
Where dimensionality of d is much higher than dimensionality of n
Why? - In high dimension space, everything is far apart. Makes it easy to rotate, scale each point.
Instead of having 100 neurons in one layer (very fat network), use 2 layers of 10 neurons each (deeper) to get the same thing.
1000 = 10 x 10 x 10
Fewer neurons, lesser to store. But deeper = slower. Wider = need more parallel hardware and can be executed.

Input = 2d space
Intermediate = 100d space
Output = 3d space

Input to the softargmax layer is called logits, which are the outputs of the final layer
L(Yhat, c) = (1/m)(sum(l(yhati, ci))), l(yhat, c) = -log(yhat[c])
l = negative log-likelihood / cross entropy 
x, c = 1, y = (1 0 0)
yhat(x) = (~1 ~0 ~0) => l( (~1 ~0 ~0) , 1) -> -log(1-) -> - 0- -> 0+
yhat(x) = (~0 ~1 ~0) => l( (~0 ~1 ~0) , 1) -> -log(0+) -> - -inf -> inf

Training a neural network
h = f(Wh x + bh)
yhat = g(Wy h + by)
theta = {Wh, bh, Wy, by}
J(theta) = L(Yhat(theta), c)
Gradient descent

dJ / dWy = dJ / dyhat * dyhat / dWy
dJ / dWh = dJ / dyhat * dyhat / dh * dh / dWh

=> backpropagation






================================================================================================================================






Week 3 Practicum - Alfredo Canziani:

Sound is a 1D signal
What if you have 2 channels (stereo)?
- still a 1D signal - time + amplitude at each point in time

Images - 2D signal, can have more channels - RGB, hyperspectral etc. But each image will have only 2 dimensions where the information is stored. The thickness (RGB etc.) does not vary across the image or between images

Properties of natural signals:
1. Stationarity - same pattern repeats a certain interval of the temporal dimension
2. Locality - information is contained in specific parts of the temporal domain. As you go further away from a point in the signal, the probability that the same/related information is contained reduces
3. Compositionality

FC layer - multiply everything by everything. Very computationally expensive.

Locality => sparsity
CNN : 5 - 3 - 1
3 connections from one layer to the next
Computations: 9 - 3

RF - receptive field
RF of last layer w.r.t. hidden layer = 3
RF of hidden layer w.r.t. input layer = 3
RF of last layer w.r.t. input layer = 5

Whole architecture sees the whole input, but intermediate layers see only the sub parts
Unnecessary computations are removed

If data doesn't show locality, sparsity cannot be used.

Stationarity => Parameter sharing
Go from 3 separate connection between layers to 3 shared connections between layers

Total computations reduced from 15 to 9 to 3

The set of 3 weights is called a kernel

Parameters sharing
- faster convergence
- better generalization
- not constrained to input size
- kernel independence => high parallellization

Connection sparsity
- reduced computation


Kernels - 1D data
layers: 5 - 3
R7 -> R2
Kernel size - 2 x 7 x 3
1D data uses 3D kernels

2D data uses 4D kernels

3x3 or 5x5 kernels typically used - empirical

Odd number of elements in the kernel => central element exists => even number of elements on both sides of the kernel
Even => there is no specific central element. Central element becomes the average of 2 elements. This could cause a low-pass kind of effect and degrade/lower the quality of data.

Zero padding - Add (kernel - 1) / 2 zeros to either side
For kernel = 3x3, zero padding = (3-1)/2 = 1 zero on either side

Standard spatial CNN:
- Multiple layers
  - Convolution
  - Non-linearity (ReLU and Leaky ReLU)
  - Pooling
  - Batch normalization
- Residual bypass connection

As you go up the layers, there is a conversion between the spatial information to the characteristic information

Pooling
Max pooling, average pooling - all kinds of pooling are wrong
Choose some information but throw away others






================================================================================================================================






Week 4 Practicum - Alfredo Canziani:

Linear algebra review

h = f(z); where z = Ax; x E Rn, z E Rm => A = m x n

a11, a12..., a1n	x1
a21, a22..., a2n	x2
.
.
am1, am2..., amn	xn	

=

--a(1)--	|
--a(2)--	|
.			x
.			|
--a(m)--	|


= 

a(1)x
a(2)x
.
.
a(m)x

= 

z1
z2
.
.
zm

= z


What is aT x , when n = 2? (T => transpose)

a and x - points in 2D plane
angle between a and x-axis = alpha
angle between x and x-axis = psi

a1x1 + a2x2 = ||a|| cos(alpha) ||x|| cos(psi) + ||a|| sin(alpha) ||x|| sin(psi)
= ||a|| ||x|| (cos(alpha)cos(psi) + sin(alpha)sin(psi))
= ||a|| ||x|| cos(psi - alpha)

cos(0) = 1 - if both vectors are aligned, max value
cos(180) = -1 - if both vectors are in opposite directions, min value

Linear layer - what is the projection of the input on each row? That is effectively what is computed. Alignment of the input with respect to each row of the matrix


OR

		|		|			  |
z = 	a1x1 +  a2x2 + ... +  anxn
		|		|			  |

Weight each column of the matrix with corresponding coefficient from the input vector

Extension to convolutions

Locality, stationarity
Weight sharing, sparsity

Toeplitz matrix - sparse matrix

Convolution is basically matrix multiplications with a lot of zeros






================================================================================================================================






Week 5 Practicum - Alfredo Canziani:

Monophonic signal - 1 channel
Stereophonic signal - 2 channels

Audio signal
x: Omega -> R^c, where c = 1 or 2 or ..., Omega is {0, 1, 2 ....} subset of N^1 => 1D signal/1D domain

How to use conv with 1d and 2d signals.

Autograd tutorial - Already completed - 03_autograd_tutorial.ipynb

Gradient descent != backpropagation

a = (1/4)sum(z)
z = 3y^2 = 3(x-2)^2

a E R

da/dx = 1/4 . 3 . 2 . (x-2) = 1.5x - 3
x = [1 2
	 3 4]
da/dx = [-1.5 0
		 1.5  3]






================================================================================================================================






Week 6 Practicum - Alfredo Canziani:		 

RNN and LSTM

Temporal data - 1D
One to many - Single input - generates sequence of words [e.g. input image - output sequence describing the image] - vec -> seq
Many to one - sequence of inputs - generates a single output [e.g. learning to execute: input python program - output is output of program] - seq -> vec
Many to many - sequence of inputs - compressed to a vector - generates sequence of outputs [e.g. language translation] - seq -> vec -> seq
Many to many - sequence of inputs - generate sequence of outputs at the same time [e.g. GPT - given a sequence/prompt, suggest the next set of words/lines] - seq -> seq

Autoregressive network - loops - previous output given as input to next time step

RNNs are similar to CNNs in the sense the same weights are shared across time steps - kind of a convolution

Take a sequence - abcdefghij...z
Create batches - 
agms
bhnt
ciou
djpv
ekqw
flrx
Batch size = 4

BPTT period T = 3
Get batch -
agms -:
bhnt  : - x[1:T]   -:
ciou -: 		    :- y[1:T]
djpv			   -:

Given a letter, predict the next letter. But do it in batches.

    y[1]	y[2]	y[T]
    bhnt	ciou	djpv		
      ^       ^       ^
h[0]  |  h[1] |  h[2] |   h[T]
-/-> RNN  -> RNN  -> RNN  -/->
      ^       ^       ^
      |       |       |
    agms    bhnt    ciou
    x[1]    x[2]    x[T]

Why does h[t] have a -/-> ? So that the gradients do not backpropagate forever. Done using .detach() in PyTorch.

Vanishing and exploding gradients:
Horizontal arrows betwen hidden states -> The previous hidden state is rotated via matrices to get the next hidden state
Based on the matrices, the gradients will either be amplified (exploding) or killed (vanishing)

RNNs were the first deep networks in the 90s. The depth was in terms of time.

In modern deep networks, how to avoid vanishing gradients? By using skip connections!
Same concept could be used with RNNs.

How to avoid vanishing gradients?
Don't propagate gradients through matrices, i.e., don't propagate forward through matrices

Instead of using 1 RNN, use 4. Gated Recurrent Neural Networks - use 4 simple RNNs merged together such that there is multiplicative interaction and not matrix interaction.


LSTM - Gated RNNs
input, don't forget, output, candidate gate
input, don't forget and output are all sigmoids - 0 to 1 - 0 means don't allow, 1 means allow
Control the output - saturated sigmoid - 1=on, 0=off - if sigmoid is at 0, internal state is not propagated out via h[t], if 1 - reverse i.e., internal state is outputted via h[t]
Control the memory - reset, keep and write - control the don't forget gate and the input gate together 
- if input gate is 0 and don't forget gate is 0 - the output of c[t] will be 0, hence reset, i.e., erase the memory
- input gate 0, don't forget 1 - output c[t] will be c[t-1] - memory is kept as is
- input gate 1, don't forget 0 - output c[t] will be influenced by input - hence write

5 steps:
1. Forward pass - compute the output of the model for a given input
2. Compute the loss
3. Zero grad - clear the cache
4. Backward - compute the partial derivatives of the loss w.r.t. the network's parameters
5. Optimizer step - take a step in the reverse direction

LSTM has 4 times the weights as the simple RNN






================================================================================================================================






Week 7 Practicum - Alfredo Canziani:		

Paper - StyleGAN - faces - approx 50 degress of freedom - although the pixel space is very high dimensional, the data manifold is in a smaller subspace
2 images - dog and bird - 
linear interpolation in pixel space => superposition - images overlaid on each other
linear interpolation in latent space => creates a bird+dog all the way till a dog+bird
Paper - Large scale GAN training for high fidelity natural images synthesis

Transformations, SR, Inpainting, caption to image

Autoencoders
Unsupervised learning

Easiest thing for an autoencoder to learn is the identity matrix
If the hidden dimensionality is lower than the input dimensionality, then it becomes harder to learn the identity matrix
Encoder can work as a compressor. e.g., image compression. This is just one way to think about this, but it is not proper.

Task of autoencoder is to reconstruct data which lives on the data manifold - a small subset of possible inputs, i.e., cannot reconstruct data which is away from the manifold
E.g., image of face with a patch blanking it out. This takes the data away from the manifold. But the autoencoder can only reconstruct data which is on the manifold. So when it takes this image with patch as input, it will reconstruct an image without a patch. So the autoencoder is basically insensitive to perturbations.

Reconstruction losses:
Binary Cross Entropy
MSE

Under/over complete hidden layer
Under-complete - hidden layer size is smaller than input size
For under-complete - cannot simply use a identity function
Autoencoder can be for compression

Over-complete - hidden layer size is greater than input size
Larger intermediate space => easier optimization
Info in the input layer and the hidden layer will still be the same, but it becomes easier for the network to manipulate the data since it has more dimensions
Becomes easy to learn the identity matrix. Hence, some constraints need to be added, i.e., bottleneck the information.
The hidden representation should only be that which the data on the manifold can have.

Even for under-complete, it is very easy to overfit. The hidden layer can simply be one neuron and the encoder and decoder can be powerful enough such that the intermediate representation is simply a map which memorizes the encoder data and the decoder can simply select the appropriate training data point from the hidden layer. So, both the under and over-complete autoencoders can overfit unless care is taken to prevent this.
Options - contrastive, regularized, architectural

Denoising autoencoder:
Choose a training data input, corrupt the input, push the data away from the data manifold, force the network to reconstruct the data which is on the manifold. Basically, learn a vector field which will push the data close to the data manifold back onto the manifold itself. The assumption is, the same kind of noise distribution is injected during training which is expected to be seen during testing.

If data is on the manifold, they will not move. If data is away from the manifold, it will move a lot, closer to the manifold. The amount of movement is the energy.

Regularized autoencoder:
Add an l-1 regularization cost term on the hidden representation
Force the network to come up with hidden representations which are short of a few dimensions
This means only a few neurons will be active at a given time

Contractive autoencoder:
Loss function = reconstruction loss + gradient of hidden representation w.r.t. input norm square
Minimize variations of the hidden layer given variations of the input - find a representation of the hidden layer which does not change that much when the input is changed a bit
The reconstruction loss penalizes insensitivity to reconstruction direction
The second term penalizes sensitivity to any direction, i.e., no assumptions over the perturbations being applied. It is simply insensitive to everything. 

Basic autoencoder
x lives in high dimension space
The autoencoder has to bring the x to z, the latent dimension, which is in a smaller dimension (e.g., a line)
The autoencoder learns how to map points in the input space to points in the latent space. How to map regions in the input space to regions in the latent space? By learning this mapping, you can go from the input space to the latent space and back. 
Denoising autoencoder take a point from the input, perturbs it and still has to go the same point in the latent space so that you can go back to the original point in the input space.
In contractive autoencoder, wiggle the input, minimize the movement in the latent space.

How can you start from the latent space, move around and get something which looks like something which belongs in the input space? Not sure exactly how the decoder behaves when we move from latent space to input space when the point on the latent space is not among the points which were used during training.






