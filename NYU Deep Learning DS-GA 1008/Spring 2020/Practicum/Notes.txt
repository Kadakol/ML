Week 1 Practicum - Alfredo Canziani:

Linear algebra

G Strang book
3blue1brown

Matrix multiplication:
Linear transformations:
	Rotation
	Scaling/Stretching
	Reflection
	Shearing
	Translation
	
Affine Transformations
	
Translation
Scaling - using diagonal matrices

Using matrices and scalars to move things around and zoom

Singular Value Decomposition

Branches of a spiral - example
2D plane
(x, y)
color is a 3rd dimension - defines which class the arms of the spiral belong to

Stretch the space such that all arms become linearly separable
Finally at convergence, we get a matrix with N rows (where N represents the number of classes)
In this case, i/p dimension is 2 (2d space). So number of columns = 2. 5 classes output, so number of rows = 5.

NNs take the space fabric and apply a transformation, which is still parameterized by matrices

Why do we need many matrices and non-linearities?
	Here, we need 2 matrices
	Input matrix = 2
	Intermediate layer = 100
	Non-linearity = positive part
	Output layer = 5
	
Without non-linearities - looks just like a 2 layer network. And 2 layer networks can only do scaling, translation, rotation, reflection and shearing






================================================================================================================================






Week 2 Practicum - Alfredo Canziani:

Neural nets - used for rotation and squashing

ANN - supervised learning - classification

3 branches of a spiral
R2 - 2D space
colors - labels

Logistic regression - linear decision boundaries
points cut across decision boundaries. Not linearly separable.

Training data:
X E R^n

X = m x n (data - m rows, n columns. In this case, n=2 for 2D space. So m x 2)
c = Column vector - m x 1 - c1, c2..., cm, where ci E {1, 2, ... K}. Here, there are 3 classes. So K=3.
Y = 1-hot encoded values of c - m x K. The 3 classes here are 100, 010, 001. So Y is of dim m x 3.

FC connected layer
         h
x -> f -> g -> y^
  Wh   Wg

h = f(Wh x + bh)
y^ = g(Wy h + by)

Affine transformations = rotations
Non-linear functions = squashing

f and g are non-linear functions

Rn -> Rd -> Rk
Where dimensionality of d is much higher than dimensionality of n
Why? - In high dimension space, everything is far apart. Makes it easy to rotate, scale each point.
Instead of having 100 neurons in one layer (very fat network), use 2 layers of 10 neurons each (deeper) to get the same thing.
1000 = 10 x 10 x 10
Fewer neurons, lesser to store. But deeper = slower. Wider = need more parallel hardware and can be executed.

Input = 2d space
Intermediate = 100d space
Output = 3d space

Input to the softargmax layer is called logits, which are the outputs of the final layer
L(Yhat, c) = (1/m)(sum(l(yhati, ci))), l(yhat, c) = -log(yhat[c])
l = negative log-likelihood / cross entropy 
x, c = 1, y = (1 0 0)
yhat(x) = (~1 ~0 ~0) => l( (~1 ~0 ~0) , 1) -> -log(1-) -> - 0- -> 0+
yhat(x) = (~0 ~1 ~0) => l( (~0 ~1 ~0) , 1) -> -log(0+) -> - -inf -> inf

Training a neural network
h = f(Wh x + bh)
yhat = g(Wy h + by)
theta = {Wh, bh, Wy, by}
J(theta) = L(Yhat(theta), c)
Gradient descent

dJ / dWy = dJ / dyhat * dyhat / dWy
dJ / dWh = dJ / dyhat * dyhat / dh * dh / dWh

=> backpropagation






================================================================================================================================






Week 3 Practicum - Alfredo Canziani:

Sound is a 1D signal
What if you have 2 channels (stereo)?
- still a 1D signal - time + amplitude at each point in time

Images - 2D signal, can have more channels - RGB, hyperspectral etc. But each image will have only 2 dimensions where the information is stored. The thickness (RGB etc.) does not vary across the image or between images

Properties of natural signals:
1. Stationarity - same pattern repeats a certain interval of the temporal dimension
2. Locality - information is contained in specific parts of the temporal domain. As you go further away from a point in the signal, the probability that the same/related information is contained reduces
3. Compositionality

FC layer - multiply everything by everything. Very computationally expensive.

Locality => sparsity
CNN : 5 - 3 - 1
3 connections from one layer to the next
Computations: 9 - 3

RF - receptive field
RF of last layer w.r.t. hidden layer = 3
RF of hidden layer w.r.t. input layer = 3
RF of last layer w.r.t. input layer = 5

Whole architecture sees the whole input, but intermediate layers see only the sub parts
Unnecessary computations are removed

If data doesn't show locality, sparsity cannot be used.

Stationarity => Parameter sharing
Go from 3 separate connection between layers to 3 shared connections between layers

Total computations reduced from 15 to 9 to 3

The set of 3 weights is called a kernel

Parameters sharing
- faster convergence
- better generalization
- not constrained to input size
- kernel independence => high parallellization

Connection sparsity
- reduced computation


Kernels - 1D data
layers: 5 - 3
R7 -> R2
Kernel size - 2 x 7 x 3
1D data uses 3D kernels

2D data uses 4D kernels

3x3 or 5x5 kernels typically used - empirical

Odd number of elements in the kernel => central element exists => even number of elements on both sides of the kernel
Even => there is no specific central element. Central element becomes the average of 2 elements. This could cause a low-pass kind of effect and degrade/lower the quality of data.

Zero padding - Add (kernel - 1) / 2 zeros to either side
For kernel = 3x3, zero padding = (3-1)/2 = 1 zero on either side

Standard spatial CNN:
- Multiple layers
  - Convolution
  - Non-linearity (ReLU and Leaky ReLU)
  - Pooling
  - Batch normalization
- Residual bypass connection

As you go up the layers, there is a conversion between the spatial information to the characteristic information

Pooling
Max pooling, average pooling - all kinds of pooling are wrong
Choose some information but throw away others






================================================================================================================================






Week 4 Practicum - Alfredo Canziani:

Linear algebra review

h = f(z); where z = Ax; x E Rn, z E Rm => A = m x n

a11, a12..., a1n	x1
a21, a22..., a2n	x2
.
.
am1, am2..., amn	xn	

=

--a(1)--	|
--a(2)--	|
.			x
.			|
--a(m)--	|


= 

a(1)x
a(2)x
.
.
a(m)x

= 

z1
z2
.
.
zm

= z


What is aT x , when n = 2? (T => transpose)

a and x - points in 2D plane
angle between a and x-axis = alpha
angle between x and x-axis = psi

a1x1 + a2x2 = ||a|| cos(alpha) ||x|| cos(psi) + ||a|| sin(alpha) ||x|| sin(psi)
= ||a|| ||x|| (cos(alpha)cos(psi) + sin(alpha)sin(psi))
= ||a|| ||x|| cos(psi - alpha)

cos(0) = 1 - if both vectors are aligned, max value
cos(180) = -1 - if both vectors are in opposite directions, min value

Linear layer - what is the projection of the input on each row? That is effectively what is computed. Alignment of the input with respect to each row of the matrix


OR

		|		|			  |
z = 	a1x1 +  a2x2 + ... +  anxn
		|		|			  |

Weight each column of the matrix with corresponding coefficient from the input vector

Extension to convolutions

Locality, stationarity
Weight sharing, sparsity

Toeplitz matrix - sparse matrix

Convolution is basically matrix multiplications with a lot of zeros






================================================================================================================================






Week 5 Practicum - Alfredo Canziani:

Monophonic signal - 1 channel
Stereophonic signal - 2 channels

Audio signal
x: Omega -> R^c, where c = 1 or 2 or ..., Omega is {0, 1, 2 ....} subset of N^1 => 1D signal/1D domain

How to use conv with 1d and 2d signals.

Autograd tutorial - Already completed - 03_autograd_tutorial.ipynb

Gradient descent != backpropagation

a = (1/4)sum(z)
z = 3y^2 = 3(x-2)^2

a E R

da/dx = 1/4 . 3 . 2 . (x-2) = 1.5x - 3
x = [1 2
	 3 4]
da/dx = [-1.5 0
		 1.5  3]
		 





