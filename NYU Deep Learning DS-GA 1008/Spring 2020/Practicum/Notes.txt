Week 1 Practicum - Alfredo Canziani:

Linear algebra

G Strang book
3blue1brown

Matrix multiplication:
Linear transformations:
	Rotation
	Scaling/Stretching
	Reflection
	Shearing
	Translation
	
Affine Transformations
	
Translation
Scaling - using diagonal matrices

Using matrices and scalars to move things around and zoom

Singular Value Decomposition

Branches of a spiral - example
2D plane
(x, y)
color is a 3rd dimension - defines which class the arms of the spiral belong to

Stretch the space such that all arms become linearly separable
Finally at convergence, we get a matrix with N rows (where N represents the number of classes)
In this case, i/p dimension is 2 (2d space). So number of columns = 2. 5 classes output, so number of rows = 5.

NNs take the space fabric and apply a transformation, which is still parameterized by matrices

Why do we need many matrices and non-linearities?
	Here, we need 2 matrices
	Input matrix = 2
	Intermediate layer = 100
	Non-linearity = positive part
	Output layer = 5
	
Without non-linearities - looks just like a 2 layer network. And 2 layer networks can only do scaling, translation, rotation, reflection and shearing






================================================================================================================================






Week 2 Practicum - Alfredo Canziani:

Neural nets - used for rotation and squashing

ANN - supervised learning - classification

3 branches of a spiral
R2 - 2D space
colors - labels

Logistic regression - linear decision boundaries
points cut across decision boundaries. Not linearly separable.

Training data:
X E R^n

X = m x n (data - m rows, n columns. In this case, n=2 for 2D space. So m x 2)
c = Column vector - m x 1 - c1, c2..., cm, where ci E {1, 2, ... K}. Here, there are 3 classes. So K=3.
Y = 1-hot encoded values of c - m x K. The 3 classes here are 100, 010, 001. So Y is of dim m x 3.

FC connected layer
         h
x -> f -> g -> y^
  Wh   Wg

h = f(Wh x + bh)
y^ = g(Wy h + by)

Affine transformations = rotations
Non-linear functions = squashing

f and g are non-linear functions

Rn -> Rd -> Rk
Where dimensionality of d is much higher than dimensionality of n
Why? - In high dimension space, everything is far apart. Makes it easy to rotate, scale each point.
Instead of having 100 neurons in one layer (very fat network), use 2 layers of 10 neurons each (deeper) to get the same thing.
1000 = 10 x 10 x 10
Fewer neurons, lesser to store. But deeper = slower. Wider = need more parallel hardware and can be executed.

Input = 2d space
Intermediate = 100d space
Output = 3d space

Input to the softargmax layer is called logits, which are the outputs of the final layer
L(Yhat, c) = (1/m)(sum(l(yhati, ci))), l(yhat, c) = -log(yhat[c])
l = negative log-likelihood / cross entropy 
x, c = 1, y = (1 0 0)
yhat(x) = (~1 ~0 ~0) => l( (~1 ~0 ~0) , 1) -> -log(1-) -> - 0- -> 0+
yhat(x) = (~0 ~1 ~0) => l( (~0 ~1 ~0) , 1) -> -log(0+) -> - -inf -> inf

Training a neural network
h = f(Wh x + bh)
yhat = g(Wy h + by)
theta = {Wh, bh, Wy, by}
J(theta) = L(Yhat(theta), c)
Gradient descent

dJ / dWy = dJ / dyhat * dyhat / dWy
dJ / dWh = dJ / dyhat * dyhat / dh * dh / dWh

=> backpropagation






================================================================================================================================






Week 3 Practicum - Alfredo Canziani:

Sound is a 1D signal
What if you have 2 channels (stereo)?
- still a 1D signal - time + amplitude at each point in time

Images - 2D signal, can have more channels - RGB, hyperspectral etc. But each image will have only 2 dimensions where the information is stored. The thickness (RGB etc.) does not vary across the image or between images

Properties of natural signals:
1. Stationarity - same pattern repeats a certain interval of the temporal dimension
2. Locality - information is contained in specific parts of the temporal domain. As you go further away from a point in the signal, the probability that the same/related information is contained reduces
3. Compositionality

FC layer - multiply everything by everything. Very computationally expensive.

Locality => sparsity
CNN : 5 - 3 - 1
3 connections from one layer to the next
Computations: 9 - 3

RF - receptive field
RF of last layer w.r.t. hidden layer = 3
RF of hidden layer w.r.t. input layer = 3
RF of last layer w.r.t. input layer = 5

Whole architecture sees the whole input, but intermediate layers see only the sub parts
Unnecessary computations are removed

If data doesn't show locality, sparsity cannot be used.

Stationarity => Parameter sharing
Go from 3 separate connection between layers to 3 shared connections between layers

Total computations reduced from 15 to 9 to 3

The set of 3 weights is called a kernel

Parameters sharing
- faster convergence
- better generalization
- not constrained to input size
- kernel independence => high parallellization

Connection sparsity
- reduced computation


Kernels - 1D data
layers: 5 - 3
R7 -> R2
Kernel size - 2 x 7 x 3
1D data uses 3D kernels

2D data uses 4D kernels

3x3 or 5x5 kernels typically used - empirical

Odd number of elements in the kernel => central element exists => even number of elements on both sides of the kernel
Even => there is no specific central element. Central element becomes the average of 2 elements. This could cause a low-pass kind of effect and degrade/lower the quality of data.

Zero padding - Add (kernel - 1) / 2 zeros to either side
For kernel = 3x3, zero padding = (3-1)/2 = 1 zero on either side

Standard spatial CNN:
- Multiple layers
  - Convolution
  - Non-linearity (ReLU and Leaky ReLU)
  - Pooling
  - Batch normalization
- Residual bypass connection

As you go up the layers, there is a conversion between the spatial information to the characteristic information

Pooling
Max pooling, average pooling - all kinds of pooling are wrong
Choose some information but throw away others






================================================================================================================================






Week 4 Practicum - Alfredo Canziani:

Linear algebra review

h = f(z); where z = Ax; x E Rn, z E Rm => A = m x n

a11, a12..., a1n	x1
a21, a22..., a2n	x2
.
.
am1, am2..., amn	xn	

=

--a(1)--	|
--a(2)--	|
.			x
.			|
--a(m)--	|


= 

a(1)x
a(2)x
.
.
a(m)x

= 

z1
z2
.
.
zm

= z


What is aT x , when n = 2? (T => transpose)

a and x - points in 2D plane
angle between a and x-axis = alpha
angle between x and x-axis = psi

a1x1 + a2x2 = ||a|| cos(alpha) ||x|| cos(psi) + ||a|| sin(alpha) ||x|| sin(psi)
= ||a|| ||x|| (cos(alpha)cos(psi) + sin(alpha)sin(psi))
= ||a|| ||x|| cos(psi - alpha)

cos(0) = 1 - if both vectors are aligned, max value
cos(180) = -1 - if both vectors are in opposite directions, min value

Linear layer - what is the projection of the input on each row? That is effectively what is computed. Alignment of the input with respect to each row of the matrix


OR

		|		|			  |
z = 	a1x1 +  a2x2 + ... +  anxn
		|		|			  |

Weight each column of the matrix with corresponding coefficient from the input vector

Extension to convolutions

Locality, stationarity
Weight sharing, sparsity

Toeplitz matrix - sparse matrix

Convolution is basically matrix multiplications with a lot of zeros






================================================================================================================================






Week 5 Practicum - Alfredo Canziani:

Monophonic signal - 1 channel
Stereophonic signal - 2 channels

Audio signal
x: Omega -> R^c, where c = 1 or 2 or ..., Omega is {0, 1, 2 ....} subset of N^1 => 1D signal/1D domain

How to use conv with 1d and 2d signals.

Autograd tutorial - Already completed - 03_autograd_tutorial.ipynb

Gradient descent != backpropagation

a = (1/4)sum(z)
z = 3y^2 = 3(x-2)^2

a E R

da/dx = 1/4 . 3 . 2 . (x-2) = 1.5x - 3
x = [1 2
	 3 4]
da/dx = [-1.5 0
		 1.5  3]






================================================================================================================================






Week 6 Practicum - Alfredo Canziani:		 

RNN and LSTM

Temporal data - 1D
One to many - Single input - generates sequence of words [e.g. input image - output sequence describing the image] - vec -> seq
Many to one - sequence of inputs - generates a single output [e.g. learning to execute: input python program - output is output of program] - seq -> vec
Many to many - sequence of inputs - compressed to a vector - generates sequence of outputs [e.g. language translation] - seq -> vec -> seq
Many to many - sequence of inputs - generate sequence of outputs at the same time [e.g. GPT - given a sequence/prompt, suggest the next set of words/lines] - seq -> seq

Autoregressive network - loops - previous output given as input to next time step

RNNs are similar to CNNs in the sense the same weights are shared across time steps - kind of a convolution

Take a sequence - abcdefghij...z
Create batches - 
agms
bhnt
ciou
djpv
ekqw
flrx
Batch size = 4

BPTT period T = 3
Get batch -
agms -:
bhnt  : - x[1:T]   -:
ciou -: 		    :- y[1:T]
djpv			   -:

Given a letter, predict the next letter. But do it in batches.

    y[1]	y[2]	y[T]
    bhnt	ciou	djpv		
      ^       ^       ^
h[0]  |  h[1] |  h[2] |   h[T]
-/-> RNN  -> RNN  -> RNN  -/->
      ^       ^       ^
      |       |       |
    agms    bhnt    ciou
    x[1]    x[2]    x[T]

Why does h[t] have a -/-> ? So that the gradients do not backpropagate forever. Done using .detach() in PyTorch.

Vanishing and exploding gradients:
Horizontal arrows betwen hidden states -> The previous hidden state is rotated via matrices to get the next hidden state
Based on the matrices, the gradients will either be amplified (exploding) or killed (vanishing)

RNNs were the first deep networks in the 90s. The depth was in terms of time.

In modern deep networks, how to avoid vanishing gradients? By using skip connections!
Same concept could be used with RNNs.

How to avoid vanishing gradients?
Don't propagate gradients through matrices, i.e., don't propagate forward through matrices

Instead of using 1 RNN, use 4. Gated Recurrent Neural Networks - use 4 simple RNNs merged together such that there is multiplicative interaction and not matrix interaction.


LSTM - Gated RNNs
input, don't forget, output, candidate gate
input, don't forget and output are all sigmoids - 0 to 1 - 0 means don't allow, 1 means allow
Control the output - saturated sigmoid - 1=on, 0=off - if sigmoid is at 0, internal state is not propagated out via h[t], if 1 - reverse i.e., internal state is outputted via h[t]
Control the memory - reset, keep and write - control the don't forget gate and the input gate together 
- if input gate is 0 and don't forget gate is 0 - the output of c[t] will be 0, hence reset, i.e., erase the memory
- input gate 0, don't forget 1 - output c[t] will be c[t-1] - memory is kept as is
- input gate 1, don't forget 0 - output c[t] will be influenced by input - hence write

5 steps:
1. Forward pass - compute the output of the model for a given input
2. Compute the loss
3. Zero grad - clear the cache
4. Backward - compute the partial derivatives of the loss w.r.t. the network's parameters
5. Optimizer step - take a step in the reverse direction

LSTM has 4 times the weights as the simple RNN






